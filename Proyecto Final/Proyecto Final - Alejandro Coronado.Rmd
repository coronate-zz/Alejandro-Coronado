---
title: "Mineria de Datos - Proyecto Final"
author: "Alejandro Coronado"
date: "17 de noviembre de 2016"
output: html_document
---

library('ggplot2')
library('ggplot2')

#Proyecto Final 
Resolver los ejercicios vistos en clase.

##Análisis Descriptivo

En esta seccin se resuelven algunos probelmas que se presnetan cuando limpiamos bases de datos y queremos hacer uns descripcion de las relacion entre nuestras variables.

* GGplot
* German Data Set
* Algas Data Set
* Berka
* XXX

***

##GGplot 

Cargando Librerias:
```{r cache=FALSE, warning=FALSE}

 library('lubridate')
 library( 'magrittr')
 library( 'ggvis')
 library( 'dplyr')
 library( 'tidyr')
 library( 'readr')
 library('rvest')
 library( 'ggplot2')
 library('stringr')
 library('ggthemes')
 library('googleVis')
 library('shiny')
 library( 'tibble')
 library( 'vcd')
 library('vcdExtra')
 library( 'GGally')
 library( 'readODS')
#library( 'readxl')
 library("RSQLite" )
 library('ggplot2movies')

```

###Boston Data Set

**Ejercicio 1**

1. Grafica `medv` usando `boxplot`, `stripchart` (*jittered dotplot*), `stem`, `density` estimate con  `rug` plot

```{r}


p<-ggplot(MASS::Boston, aes(medv)) 

p + geom_histogram() + ylab("") + xlab("Valor medio de las casas (1000s USD)")
#Density Chart con Rug
p +  geom_density() + geom_rug()+  ylab("") + xlab("Valor medio de las casas (1000s USD)")
#Stem chart
p +  geom_bar() + ylab("") + xlab("Valor medio de las casas (1000s USD)")


#Boxplot
p<-ggplot(MASS::Boston, aes( 'Valor medio de las casas', medv ))
p +  geom_boxplot() + ylab("") + xlab("Valor medio de las casas (1000s USD)")
#Stripchart
p +  geom_jitter() + ylab("") + xlab("Valor medio de las casas (1000s USD)")



```



¿Qué observas en cada una de estas gráficas?
* La primera grafica geom_histogram nos da una idea de las distribucion del valor medio de las casas en el dataSet. Vemos que la mayor concentración de precios esta en 20,000 dolares.
* Geom_density confirma la distribución del valor medio de las casas y vemos que estos precios se comportan casi como una distribución normal
* geom_bar es similar a geom_histogram pero ahora los datos no se encuentran agrupados en intervalos sino que cada uno es represnetado por su propio valor.
* geom_boxplot Nos muestra que la media de los datos esta en 20,000 dolares y que la mayor concentración de la infromación esta en esta zona. Asimismo nos muestra que tenemos varios outliers con casas con valores muy altos.
*geom_jitter nos muestra la distribución de los datos en una grafica de puntos donde cada punto representa el valor de cada casa. Se crea una distorción minima para evitar que los datos hagan overlapping.

¿Qué observas en cada una de estas gráficas?

2. En la gráfica de las 14 variables mostrada arriba ¿Cómo describirías las distribuciones? 
  ¿Para cuales variables sería mejor utilizar `boxplot`? ¿Por qué?

  
Usando Boxplot queremos tener una mejor intuición sobre la distribución de los datos asi como como conocer la media y excluir los datos atipicos. Las variables de tax, nox e indus son buenos candidatos para este analisis data su stribución asimétrica. Es dificil saber sobre que valores se encuentran concentrdos estos datos, cual es su media y es muy psible que tengamos observaciones atípicas.

**Ejercicio 2** 
###Movies Data Set
1. ¿Qué puedes decir de esta gráfica?



```{r,  warning=FALSE}
ggplot(ggplot2movies::movies, aes(x=length)) + 
    xlim(0, 180) +
    geom_histogram(binwidth = 1) +
    xlab("Duración de películas en minutos") + ylab("") 
```



Tenemos dos concentraciones en los datos, una alrededor de los 15 minutos y otra cerca de los 80, 90 minutos. Esto sucede porque estamos considerando dos tipos de peliculas los corto metrajes y las producciones de pelicula. Cada distribucion parece ser normal con diferente media y una varianza mayor en el tipo peliculas.
2. ¿Cómo la modificas para agregar más *ticks*?
Usando breaks=seq(0, 150, by=.5) dentro de geom_histogram

```{r,  warning=FALSE}
ggplot(ggplot2movies::movies, aes(x=length)) + 
    xlim(0, 180) +
    geom_histogram(breaks=seq(0, 150, by=.5)) +
    xlab("Duración de películas en minutos") + ylab("")  
    #scale_x_continuous(breaks = seq(0,180,10))

```


3. Haz una gráfica que muestre que los picos a los 7 y 90 minutos existían antes y después de 1980

Se proponen diferentes soluciones usando colour, facet_grid y filtros.

```{r,  warning=FALSE}
data<-ggplot2movies::movies
data['is1980']<-data$year>1980
p=ggplot(data,   aes(x=length, colour=is1980))  
    p  + xlim(0, 180) + geom_histogram(binwidth = 1)  +xlab("Duración de películas en minutos despues de 1980") + ylab("")
    
p=ggplot(data, aes(length))
    p+ geom_bar() + xlim(0, 180) +  facet_grid(~ is1980 )
    
p=ggplot(data, aes(length, fill=factor(year)))
    p+ geom_bar() +  xlim(0, 180) 

p=ggplot(subset(data, year>=1980), aes(x=length))  
    p + xlim(0, 180) + geom_histogram(binwidth = 1) + xlab("Duración de películas en minutos despues de 1980") + ylab("") 
pp=ggplot(subset(ggplot2movies::movies, year<1980), aes(x=length))  
    pp + xlim(0, 180) + geom_histogram(binwidth = 1) + xlab("Duración de películas en minutos antes de 1980") + ylab("") 

```



4. Existe la variable `Short` que indica si la película es "corta" ¿Qué gráfica puedes  hacer para
   ver que criterio se utilizó para definir esta variable y cuáles están mal clasificadas?
* Para hacer un pequeño analisis primero obeservamos con colores las graficas mal clasificadas, despues utilizamos un filtro con condiciones. Queremos elegir las observaciones con short=1 pero cuya duración es mayor a 45 y las variables short=0 con duración menor a 45 minutos. 

```{r,  warning=FALSE}
data<-ggplot2movies::movies
x2<-seq(1, 58788)


p=ggplot(ggplot2movies::movies,   aes(length, fill=factor(Short)))  
    p + xlim(0, 200)+ ylim(0, 200) + geom_bar() +  xlab("Pelicula ID") + ylab("Duracion de la pelicula") 

p=ggplot(ggplot2movies::movies,   aes(x=x2, y=length, colour=Short))  
    p + xlim(0, 58788)+ ylim(0, 200) + geom_point() +  xlab("Pelicula ID") + ylab("Duracion de la pelicula") 

p=ggplot(subset(ggplot2movies::movies, Short>0 & length>45), aes( x=year, y=length,  label=as.character(title)))  
     p + xlim(1900, 2006)+ ylim(0, 200) + geom_point() +  xlab("Pelicula año") + ylab("Duracion de la pelicula")  + geom_text(size=2)
     

p=ggplot(subset(ggplot2movies::movies, Short<=0 & length<45), aes( x=year, y=length, label=as.character(title)))  
     p + xlim(1900, 2006)+ ylim(0, 200) + geom_point() +  xlab("Pelicula año") + ylab("Duracion de la pelicula")  + geom_text(size=2)

```
    



** Ejercicio 3 **
1. Agrega *alpha-blending* ¿Qué pasa con los  *outliers*? ¿Diferentes valores funcionan mejor?

* Cuando aumentamos el valor alpha vemos una mayor cantidad de puntos traslucidos señalandonos cuales valores podrian ser considerados outliers dado el criterio alpha
```{r,  warning=FALSE}

ggplot(ggplot2movies::movies, aes(votes, rating, alpha=.005)) + geom_point() + ylim(1,10)

ggplot(ggplot2movies::movies, aes(votes, rating, alpha=.2)) + geom_point() + ylim(1,10)

ggplot(ggplot2movies::movies, aes(votes, rating, alpha=70)) + geom_point() + ylim(1,10)
```

2. ¿Cómo se ve la gráfica si  remueves las películas con menos de 100 votos?
* Se eliminan valores extremos sobre las calificaciones y reduciomos el rango de y. Tambien es mas facil reconocer una distribución con concentración en los valores 5 y 7.5

```{r,  warning=FALSE}
data<-subset(ggplot2movies::movies,  votes>100)

ggplot(data, aes(votes, rating, alpha=.3)) + geom_point() + ylim(1,10)+ xlim(0,2000)

```

3. ¿Cómo si remueves todas las películas que tienen un *rating* arriba de 9?
* La gráfica se parece a la anterior porque existen muchas películas con calificaciones muy altas pero que fueron calificadas por pocas personas. Cuando pocas personas califican entonces es probable que su calificación sea mayor a la que realmente deberia tener.

```{r,  warning=FALSE}
data<-subset(ggplot2movies::movies,  rating<9)

ggplot(data, aes(votes, rating) ) + geom_point() + ylim(1,10)

```

###Cars93 Data Set
**Ejercicio 4**

- ¿Cuál es el *outlier* de la izquierda?

```{r,  warning=FALSE}
ggplot(MASS::Cars93, aes(Weight, MPG.city)) + geom_point() +
    geom_smooth(colour="green") + ylim(0,50)
```

El outlier es Civic, el carro mas confiable que podrías tener.

```{r,  warning=FALSE}
ggplot(MASS::Cars93, aes(Weight, MPG.city, label=as.character(Model))) + geom_point() +
    geom_smooth(colour="green") + ylim(0,50)+ geom_text()
```



###Boston Data Set
**Ejercicio 5**

```{r, fig.height=13, fig.width=13,   warning=FALSE}
MASS::Boston %>%
    select(-rad,-chas) %>%
    ggpairs(title="Boston Dataset", diag=list(continuos='density', axisLabels='none'))
```


1. ¿Cuáles están positivamente correlacionadas con `medv`?
* rm, dis, black

2. La variable `crim` (tasa de crímenes per cápita) tiene *scatterplots* con forma inusual,
  donde los valores más altos de `crim` sólo ocurren para una valor de la otra variable
  ¿Qué explicación le puedes dar?
 
* Los crimene se realizan en zonas especificas, si vemos la grafica de crim vs dis vermos que a una menor distancia encontramos amyor cantidad de crimenes percapita y una mayor concentracion de observaciones. Esto quiere decir que tenemos mayor crimenes y mayores observaciones en zonas cercanas a los centros (zonas) comerciales.
Las zonas comerciales deben tener caracteristicas en comun como numero de habitaciones(edificios), mayor edad, menor proporcion de zonas residenciales.
 
3. Hay varias formas en los *scatterplots*, escoge 5 y explica como las interpretas?


* dis vs zn: Vemos que hay zonas residenciales concentradas en dos partes, unas relativamente cerca de los centros comerciales y otra mucho mas alejada de los centros comerciales tal vez son las zonas residenciales.
* dis vs rm: meintras aumenta la distancia las personas pueden pagar casas con mayor numero de cuartos. Esto sucede porque los terrenos son mas baratos cuando están lejos de las zonas comerciales.
* nox vs indus: entre mayor sea la cantidad de non-reatail commerce mayor es la concentraciòn de contaminantes esto puede explicarse si los comercios de non-retail son productores o fabricas.
* age vs nox: Las zonas con mayor edad tambien son las zonas con mayor cantidad de non-retail commerce. Esto podría confirmar nuestro supuesto de que se tratan de zonas insdutriales y por lo tanto fueron establecidas mucho antes que las zonas comerciales o residenciales.


**Ejercicio 6**

- Usando el `Boston` *dataset* realice un `pcp`. Trate de resaltar las características que ha observado en los 
ejercicios anteriores. Piensa como le hiciste

```{r}
data<-MASS::Boston
data['grupos']<-cut_interval(data$medv, 4)
ggparcoord(data, columns = 1:7, groupColumn = "grupos", scale='center')
```


**Ejercicio 7**

Crea en tu carpeta las carpetas `german` y `algas`, dentro de ellas crea los archivos 
`00-load.R`, `01-prepare.R` y `02-clean.R` y `run.R`
En estos archivos pondrás, respectivamente el código para ejecutar los *pipelines* siguientes de los 
casos de estudio.


***


##German Data Set

Primero debemos cargar los metadata para poder trabajar con los nombre de las columnas y decodificar nuestros datos

```{r}
## German credit ---------------------------------------------------------------

## Nombres de columnas ---------------------------------------------------------
german_colnames <- c('Status of existing checking account',
                     'Duration in month',
                     'Credit history',
                     'Purpose',
                     'Credit amount',
                     'Savings account/bonds',
                     'Present employment since',
                     'Installment rate in percentage of disposable income',
                     'Personal status and sex',
                     'Other debtors / guarantors',
                     'Present residence since',
                     'Property',
                     'Age in years',
                     'Other installment plans',
                     'Housing',
                     'Number of existing credits at this bank',
                     'Job',
                     'Number of people being liable to provide maintenance for',
                     'Telephone',
                     'foreign worker',
                     'good_loan'
)

## Códigos ---------------------------------------------------------------------
german_codes <- list('A11'='... < 0 DM',
                     'A12'='0 <= ... < 200 DM',
                     'A13'='... >= 200 DM / salary assignments for at least 1 year',
                     'A14'='no checking account',
                     'A30'='no credits taken/all credits paid back duly',
                     'A31'='all credits at this bank paid back duly',
                     'A32'='existing credits paid back duly till now',
                     'A33'='delay in paying off in the past',
                     'A34'='critical account/other credits existing (not at this bank)',
                     'A40'='car (new)',
                     'A41'='car (used)',
                     'A42'='furniture/equipment',
                     'A43'='radio/television', 'A44'='domestic appliances', 'A45'='repairs',
                     'A46'='education', 'A47'='(vacation - does not exist?)',
                     'A48'='retraining', 'A49'='business', 'A410'='others', 'A61'='... < 100 DM',
                     'A62'='100 <= ... < 500 DM', 'A63'='500 <= ... < 1000 DM',
                     'A64'='.. >= 1000 DM', 'A65'='unknown/ no savings account',
                     'A71'='unemployed', 'A72'='... < 1 year', 'A73'='1 <= ... < 4 years',
                     'A74'='4 <= ... < 7 years', 'A75'='.. >= 7 years', 'A91'='male : divorced/separated',
                     'A92'='female : divorced/separated/married',
                     'A93'='male : single',
                     'A94'='male : married/widowed',
                     'A95'='female : single',
                     'A101'='none',
                     'A102'='co-applicant',
                     'A103'='guarantor', 'A121'='real estate',
                     'A122'='if not A121 : building society savings agreement/life insurance',
                     'A123'='if not A121/A122 : car or other, not in attribute 6',
                     'A124'='unknown / no property',
                     'A141'='bank', 'A142'='stores',  'A143'='none', 'A151'='rent', 'A152'='own',
                     'A153'='for free', 'A171'='unemployed/ unskilled - non-resident',
                     'A172'='unskilled - resident', 'A173'='skilled employee / official',
                     'A174'='management/ self-employed/highly qualified employee/ officer',
                     'A191'='none', 'A192'='yes, registered under the customers name',
                     'A201'='yes', 'A202'='no'
)

```


**Ejercicio 8**

- Crea una función `load` en `utils.R` en tu carpeta, que descargue, si y sólo si no existe
un archivo `german.rds`. Si no existe, descarga y guarda el archivo.

```{r}
loadGerman<-function()
{
    print('Cargando datos de German Data')
    tryCatch(
        {
            print('El archivo existe')
            data<-readRDS( 'german.rds')
            #Intentaremos llamar a nuestra archivo
            
            

        }
    , error=function(err)
        {
        print('El archivo sera descargado')
        german_url <- paste('http://archive.ics.uci.edu/ml',
                            '/machine-learning-databases/statlog',
                            '/german/german.data',
                            sep='')

        german_data <- read_delim(german_url,
                                  col_names=FALSE,
                                  delim = " ")
        print( head(german_data))
        saveRDS(german_data, 'german.rds')

        }
    )
}
```



```{r}
loadGerman()
german_data<-readRDS( 'german.rds')
colnames(german_data) <- german_colnames
german_data$good_loan <- as.factor(
                          ifelse(
                            german_data$good_loan == 1, 
                            'GoodLoan', 
                            'BadLoan'
                            )
                          )

```



**Ejercicio 9**
- Crea una función `german_decode` en un archivo `utils.R` dentro de tu carpeta, 
esta función debe de utilizar `german_codes` (en el archivo `metadata.R`) para 
decodificar los elementos  de todas las columnas (por ejemplo `A201` -> `yes`)

- Utiliza `dplyr` para decodificar todas las columnas de `german_data`



```{r}

german_decode<-function(german_data, german_codes)
{
    german_aux<-german_data
    gercolnames<-colnames(german_data)
    for(i in gercolnames )
    {
        print(i)
        for(j in names(german_codes))
        {
        german_aux[i][ german_aux[i]==j] <-german_codes[j]
        }

    }
    german_aux<-as.data.frame(german_aux)
    #Algunos valores regresaran como listas
    for(i in colnames(german_data))
    {
        german_aux[i]<-unlist(german_aux[i])

    }
    return(german_aux)

}

```

```{r}
german_data<- german_data %>% german_decode(german_codes)
head(german_data)
```



**Ejercicio 10**

- ¿Hay algo raro con los datos de préstamo?
* Las variables son cadenas de texto y no pueden ser graficadas o analizadas en el estado en el que se encuentran.
Algunas columna contienen mas de una variable por lo que es dificil saber el valor real de estas variables.
Algunas variables numéricas aparecen como texto.

- ¿Cuál crees que debería ser la distribución del resultado del 
  préstamo `Good_Loan` respecto a `Credit history`?
* Deberiamos encontrar una concentración diferente para los tipos de Credit history para los good loans y los bad loans, por ejemplo los bad loans deberían tener una mayor cantidad de delay in paying off in time o not existing credits.

- Grafícalo y comenta tus resultados.
```{R}
p<-ggplot(german_data, aes(good_loan, fill=as.factor(as.character(german_data$`Credit history`)) ))
p +  geom_bar(position="fill") + ylab("") + xlab("Composición de los prestamos") +scale_fill_discrete("Credit history")
```



* Como se muestra la composicion de los BadLoan y GoodLoan son muy similares pero podemos observar la principal diferencia se encuentra en que los Good Loan tienen una mayor proporcion de cuentas con otros creditos en otros bancos. Asimismo los BadLoan tiene una mayor numero de cuentas que no han pedido creditos o todos los creditos han sido pagados de manera irregular.

```{r}

german_clean_colnames<-function(german_colnames)
{
    for( i in 1:length(german_colnames ))
    {


        cadena<-strsplit(german_colnames[i], ' ' )[[1]]
        newName<-''
        if(!is.na(cadena))
        {
            
            for(j in cadena)
            {

                if(newName=='')
                {
                    newName<-j
                }
                else
                {
                newName<-paste( newName , j, sep = '_')
                }

            }
            german_colnames[i]<-newName
            print(german_colnames[i])
        }

        #------------- repetir con / ------


        cadena<-strsplit(german_colnames[i], '/' )[[1]]
        newName<-''
        if(!is.na(cadena))
        {


            for(j in cadena)
            {

                if(newName=='')
                {
                    newName<-j
                }
                else
                {
                newName<-paste( newName , j, sep= '_')
                }

            }
            german_colnames[i]<-newName
            print(german_colnames[i])
        }


    }
    return(german_colnames)
}

```

```{r, echo=FALSE }
german_colnames<-german_clean_colnames( german_colnames )
colnames(german_data)<-german_colnames
```
**Ejercicio 11**

Revisa `german_data` con `summary()`, reporta alguna anomalía.


```{r}
summary(german_data)
```


No hay anomalias, la unica observación es que podriamos transformar ciertas variables a un formato numerico para poder obtener estadisticas descrptivas como por ejemplo el Personal_status_and_Sex o Housing.

**Ejercicio 12**

Asegurar que el *dataset* esté en forma *tidy*. Guarda esto en `german-tidy.rds`

* Para que nuestro archivo este en formato tidy es necesario separar la variable Personal_status_and_Sex pues tenemos dos valores dentro de una misma columna

```{r}
cadena<-strsplit( german_data$Personal_status_and_sex, ':')

sex<-list()
status<-list()
for(i in cadena)
{
  
    sex<-append(sex, i[1])
    status<- append(status, i[2])
}

german_data['sex']<-sex
german_data['status']<-status
german_data$Personal_status_and_sex<-NULL


saveRDS(german_data, 'german-tidy.rds')

```
 
 
```{r}
head(  readRDS('german-tidy.rds')  )
```

***
##Algas Data Set

**Cargando Metadata**
```{r}
## Algas -----------------------------------------------------------------------

## Nombre de columnas ----------------------------------------------------------
algas_colnames <- c('season',
                    'river_size',
                    'fluid_velocity',
                    'max_PH',
                    'min_O2',
                    'Cl',
                    'NO3',
                    'NH4',
                    'oPO4',
                    'PO4',
                    'Chla',
                    paste('a', seq(1:7), sep="")
)

```

** Ejercicio 13**

- Repite los pasos realizados para `german.data` con `algas`
- No te olvides de remover los `_` en las variables `river_size` y `fluid_velocity`
- Revisa con `summary()`, reporta alguna anomalía.

```{r}

print('Cargando nombre de Columnas')
loadAlgas<-function()
{
    print('Cargando datos de German Data')
    tryCatch(
        {
            print('El archivo existe')
            data<-readRDS( 'algas.rds')
            #Intentaremos llamar a nuestra archivo
            
            

        }
    , error=function(err)
        {
        print('El archivo sera descargado')
        algas_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/coil-mld/analysis.data'

        algas <- read_csv(algas_url, 
                  col_names = algas_colnames,
                  na = 'XXXXXXX')
        print( head(algas))
        saveRDS(algas, 'algas.rds')

        }
    )
}


#En el caso de algas no es necesario hacer un decode para los valores de las columnas

print('Modificando Nombre de Columnas')

algas_clean_colnames<-function(algas_colnames)
{
    for( i in 1:length(algas_colnames ))
    {


        cadena<-strsplit(algas_colnames[i], ' ' )[[1]]
        newName<-''
        if(!is.na(cadena))
        {
            
            for(j in cadena)
            {

                if(newName=='')
                {
                    newName<-j
                }
                else
                {
                newName<-paste( newName , j, sep = '_')
                }

            }
            algas_colnames[i]<-newName
            #print(algas_colnames[i])
        }

        #------------- repetir con / ------


        cadena<-strsplit(algas_colnames[i], '/' )[[1]]
        newName<-''
        if(!is.na(cadena))
        {


            for(j in cadena)
            {

                if(newName=='')
                {
                    newName<-j
                }
                else
                {
                newName<-paste( newName , j, sep= '_')
                }

            }
            algas_colnames[i]<-newName
            #print(algas_colnames[i])
        }


    }
    return(algas_colnames)
}


```

```{r,  }
loadAlgas()
algas<-readRDS('algas.rds')
algas_colnames<-algas_clean_colnames(algas_colnames)
head(summary(algas))
```



¿Por qué la columna `NO3` **no** es numérica?
*  Porque tenemos algunas observaciones donde tenemos mas de un punto de los numeros. 


**Ejercicio 13**

- Es importante en la etapa de exploración, poder generar varias gráficas de manera automática 
  y simple para analizarlas visualmente y tener una idea de los datos. 
- Crea una función que genere los tipos de gráfica para cada par de variables del `data.frame` (en realidad es un `tibble`).
  Esta función debe de recibir dos parámetros, uno que indique si genera todas las combinaciones 
  de dos variables o recibe una lista de variables en las cuales generar las combinaciones.
- Guárdala en `utils.R`. 
- Crea en `03-eda.R` en ambas carpetas: `algas` y `german`.

**El siguiente Codigo guarda las graficas para dos tipo de variables, Discretas y continuas. Tambien contempla si el usuario esta agregando una lista o si se desea imrprimir y gusardar solo algunas variables.**

```{r}

graficasIniciales<-function( data, lista=NULL )
{
    cont=0
    variablesDiscretas<-list()
    variablesContinuas<-list()

    if(!is.null(lista))
    {
       print('-----Generando graficas para variables en lista')
        for(i in lista)#Comprobando nombres en lista
        {
            if(is.na(match(i, colnames(data))))
            {
                print(paste('VALOR DE LISTA NO VALIDO ', i, sep=''))
                return('NA')
            }
        }
        for(i in lista)
        {
            print(paste('Graficas individulaes de ', i))


            if(is.character(data[i][[1,1]]))
            {
                variablesDiscretas<-c(variablesDiscretas , i)
                plotx<-ggplot(data, aes(data[[i]])) +
                    geom_bar() + xlab(i)
                ggsave( paste(paste('Grafica_Barras_', i, sep=''),'.png', sep=''), plotx)
                print(plotx)
            }
            else
            {

                #Grafica de Densidad para variables continuas
                variablesContinuas<-c(variablesContinuas, i)
                plotx<-ggplot(data, aes(data[i])) +
                       geom_density() + geom_vline(   xintercept = mean(as.numeric(unlist(data[i])), na.rm=TRUE  )  ) + xlab(i)
                ggsave( paste(paste('Grafica_Densidad', i, sep=''),'.png', sep=''), plotx)
                print(plotx)
                cont=cont+1

                #Grafica Q-Q para variables continuas
                plotx<-ggplot(data, aes(sample=data[i])) + stat_qq()
                print(plotx)
                ggsave( paste(paste('Grafica_QQ_', i, sep=''),'.png', sep=''), plotx)
                cont=cont+1

                #Grafica Boxplot para variables continuas
                plotx<-ggplot(data, aes(i, data[i])) + geom_boxplot()
                print(plotx)
                ggsave( paste(paste('Grafica_Boxplot_', i, sep=''),'.png', sep=''), plotx)
                cont=cont+1
            }
        }
    }
    else
    {
        print('----- Generando graficas para todas las variables')
        for(i in colnames(data))#Comprobando nombres en lista
        {
            print(paste('Graficas individulaes de ', i))
            #Grafica distribucion con media

            if(is.character(data[i][[1,1]]))
            {
                #Grafica de Barras para Varibales Discretas
                variablesDiscretas<-c(variablesDiscretas , i)
                plotx<-ggplot(data, aes(data[[i]])) +
                    geom_bar() + xlab(i)
                print(plotx)
                ggsave( paste(paste('Grafica_Barras_', i, sep=''),'.png', sep=''), plotx)
                cont=cont+1

            }
            else
            {
                #Grafica de Densidad para variables continuas
                variablesContinuas<-c(variablesContinuas, i)
                plotx<-ggplot(data, aes(data[i])) +
                    geom_density() + geom_vline(   xintercept = mean(as.numeric(unlist(data[i])), na.rm=TRUE  )  ) + xlab(i)
                ggsave( paste(paste('Grafica_Densidad_', i, sep=''),'.png', sep=''), plotx)
                print(plotx)
                cont=cont+1

                #Grafica Q-Q para variables continuas
                plotx<-ggplot(data, aes(sample=data[i])) + stat_qq()
                print(plotx)
                ggsave( paste(paste('Grafica_QQ_', i, sep=''),'.png', sep=''), plotx)
                cont=cont+1

                #Grafica Boxplot para variables continuas
                plotx<-ggplot(data, aes(i, data[i])) + geom_boxplot()
                print(plotx)
                ggsave( paste(paste('Grafica_Boxplot_', i, sep=''),'.png', sep=''), plotx , width = 13, height = 13)
                cont=cont+1

            }

        }


    }
    data2<-NULL
    print(paste('VAR CONT: ', variablesContinuas))
    for(i in variablesContinuas)
    {
        
        if(is.null(data2))
        {
            data2<-data[i]
        }
        else
        {
            data2<-cbind(data2, data[i])
        }
    }
    
    if( length(variablesContinuas)>=2 )
    {
    print(   paste( paste( 'Existen ' , length(variablesContinuas) ), ' variables Continuas') )
    #Grafica GGPAIR
    plotx<-ggpairs(data2 )
    print(plotx)
    print('guardando')
    ggsave( paste(paste('Grafica_GGPairs_', i, sep=''),'.png', sep=''), plotx , width = 13, height = 13 )
    }
    else
    {
    print('No hay suficientes variables para GGpair')
    }
    return('NA')
}

```


```{r}
graficasIniciales(algas,colnames(algas)[c(3,4,5,6)] )

```



**Ejercicio 14**

- Genera un reporte para ambos conjuntos de datos el estado de los valores missing.
- Muestra la matriz de correlación faltante en una gráfica.
- ¿Qué puedes entender?

```{r}
print('Observaciones con NAs')
algas_con_NAs <- algas[!complete.cases(algas),]
head(algas_con_NAs)

print('Numero de NAs por Observacion')
apply(algas, 1, function(x) sum(is.na(x)))

print('Seleccion de variables con un numero de NAs mayor a X')
algas[apply(algas, 1, function(x) sum(is.na(x))) > 2,]


```

```{r}

indices_con_Nas<-function(data, num )
{
    if(num>= 1)
    {
        print('El criterio de eliminacion no puede ser mayor a uno')
        return(NULL)
    }
    else
    {
        indices<-colnames(data)
        longitud<-length( indices )
  
        indRechazar<-round(longitud*num)
        observacionesNA<-apply(algas, 1, function(x) sum(is.na(x)))
     
        #print(observacionesNA)
        indices<-observacionesNA > indRechazar
        return(indices)
    }
    
}
```

* Esta funcion regresa todas las observaciones que tienen un numero de NAs por observación mayores a una fracción del total de variables en el data Set

```{r}
#Observaciones con un numero de NA's mayor al 20% de las variables
indices_con_Nas(algas, .2)

```

**Ejercicio 15**

- Una estrategia es rellenar los valores faltantes con alguna medida de centralidad.
    - Media, mediana, moda, etc.
- Para variables distribuidas normalmente, esta opción es la mejor.
- Pero para variables *skewed*  o con *outliers* esta decisión puede ser desastrosa.
- Por lo tanto, esta estrategia no se debe de utilizar salvo una exploración previa de las variables.

#### Ejemplo

- ¿A qué variables le puedes de `algas` le puedes aplicar este procedimiento?
* De acuerdo a las siguiente graficas podemos ver que no todas las variables se comportan como una normal. En realidad sólo max_PH y min_O2 parecen tener una distribucion masomenos normal. En caso de elegir la media o la moda en los casos que no tienen distribución normal es probable que estemos tomando un valor que realmente no existe en la miuestra y por lo tanto es poco probable que exista en la realidad.

```{r}
for( i in colnames(algas))
{
   tryCatch(
    {
    print( ggplot(algas, aes( algas[i] ) ) + geom_density()  + xlab(i))
    
    }
    ,error=function(err)
    {
        print(paste( 'No se pudo graficar: ', i))
    })
}
```

- ¿Qué puedes decir de `german_data`?
* Al igual que con el data Set de Algas no tenemos muchas variables que se comporten con una distribucion normal por lo tanto tomar el valor central para sustituir a los varoles NA no seria una buena estrategia.
 En particular duration_in_month y number_of_existing_credits parecen tener dos concentraciones de datos con diferentes medias.
```{r}
for( i in colnames(german_data))
{
   tryCatch(
    {
    print( ggplot(german_data, aes( german_data[i] ) ) + geom_density()  + xlab(i))

    }
    ,error=function(err)
    {
        print(paste( 'No se pudo graficar: ', i))
        
    })
}
```

- A las variables que no se les puede aplicar, explica por qué no.
* Estas Variables no tienen  una distribución normal o uniforme por lo que elegir la media para sustituir los valores NA con este numero no nos daria una buena aproximación del valor real de estas variables.  Tomemos el caso extremo de una variable binaria, el promedio siempre estara entre 0 y 1 pero este promedio no necesariamente existe como una observación

- Esta decisión debe de ser reproducible, agrega a `utils.R` una función que impute 
en las variables con  `NAs` el valor central (`median` si es numérica, `moda` si es categórica).
La función debe de tener la siguiente firma:

```{r}
print('Texto adicional')
problematic_rows <- problems(algas)$row

algas[problematic_rows,] <- algas %>% 
    slice(problematic_rows) %>% 
    unite(col="all", -seq(1:6), sep = "/", remove=TRUE) %>%
    extract(all, into=c("NO3", "NH4", "resto"), regex="([0-9]*.[0-9]{5})([0-9]*.[0-9]*)/(.*)/NA", remove=TRUE) %>%
    separate(resto, into=names(algas)[9:18], sep="/", remove=TRUE)    


algas$NO3<- as.numeric(algas$NO3)
algas$NH4<-as.numeric(algas$NH4)
algas$oPO4<-as.numeric(algas$oPO4)
algas$PO4<-as.numeric(algas$PO4)
algas$Chla<-as.numeric(algas$Chla)
algas$a1<-as.numeric(algas$a1)
algas$a2<-as.numeric(algas$a2)
algas$a3<-as.numeric(algas$a3)
algas$a4<-as.numeric(algas$a4)
algas$a5<-as.numeric(algas$a5)
algas$a6<-as.numeric(algas$a6)
algas$a7<-as.numeric(algas$a7)

print('FUNCION IMPUTAR VALOR CENTRAL')
imputar_valor_central <- function(data, colnames)
{
    for(i in colnames)
    {
        if(is.numeric(data[[i]][1]) ||is.numeric(data[[i]][2]) ) #Valores numericos
        {
        print(paste('VALOR NUMERICO', i, sep=' ')) 
        print('DATA SET')
        print( data[[i]] )
        media <-mean( as.numeric(as.character( data[[i]])), na.rm = TRUE)
        print(media) 
        data[[i]][is.na(data[[i]])]<-media
        
        }
        else
        {   print('VALOR DISCRETO')
            moda<-''
            maximo<-0
            valores<-unique(data[[i]])
            
            #Print obtención de MODA
            print(valores)
            for(v in valores)
            {
                
                
                cont<-0
             for(j in 1:length(data[[i]]))
             {
                 #print(paste( paste( paste(paste(paste('Valor i: ', i), '  Valor j: '), j), '  Valor v: '  ), v))
                 #print(data[[i]][j])
                if(!is.na(data[[i]][j]) && !is.na(v) )
                {
                    #print(paste('value' , data[[i]][j] ) )
                     if(data[[i]][j]==v) 
                    {
                         cont=cont+1
                     }
                }
             }
             if(cont>maximo)
                 {
                 moda<-v
                 maximo<-cont
                 }
            }
            data[[i]][is.na(data[i])]<-moda
        }
    }
    return(data)
}




```


```{r}
index<-!complete.cases(algas)

algas[index,]
algas_central<-imputar_valor_central(algas, colnames(algas))

algas_central[index,]


```



**Ejercicio 16**
Crea una función que sustituya los `NAs` con el valor dado por la 
regresión lineal recién calculada (No automatices la regresión lineal) usando la
siguiente firma

```{r}
algas %>%
    select(-c(1:3)) %>%
    cor(use="complete.obs") %>%
    symnum()

imputar_valor_lm <- function(var_dependiente, var_independiente, modelo)
    {
    
    
    intercepto<-modelo$coefficients[1]
    beta<-modelo$coefficients[2]
    
    lmResult <-intercepto + (beta*var_independiente[is.na(var_dependiente)])
    print(paste('Resultados Regresion  : ' , lmResult))
    
    
    var_dependiente[is.na(var_dependiente)]<-lmResult 
    return(var_dependiente)

    }
```

* Utilizando esta matriz podemos ver que podemos predecir el valor para PO4 utilizando las variables de Cl ya que ambas estan correlacionados y por lo tanto el valor de los coeficientes debe ser significativo. Podríamos usar oPO4 pero es probable que las variables tengan NA en las mismas posiciones y por lo tanto hacer regresiones no sería eficiente. 

```{r}
index<-is.na(algas$PO4)
cat('\nValores Vacios')
algas$PO4[index]

algas_lm<-algas
modelo<-lm(algas_lm$PO4 ~  algas_lm$Cl )
algas_lm$PO4 <-imputar_valor_lm(algas_lm$PO4, algas_lm$Cl, modelo)
cat('\n\nValores Calculados\n')
algas_lm$PO4[index]


```



**Ejercicio 17** 
* Es importante normalizar los valores numéricos antes de calcular las distancias. ¿Por qué?
Porque dos variables podrian ser muy parecidas y tener un comportamiento muy similar pero estar a diferentes escalas. Como estamos midiendo las distancias entonces la correlación se perdera por los numeros altos en un escala vs numeros pequeños en una escala menor. Un ejmplo sería tener la misma mariable medida en km y en centimetros. Si normalizamos tendremos una distancia igual 0 pero si no normalizamos las distancia aparecera muy grande.


**Ejercicio 18** 

- Implementen una función que impute por similitud con la firma

```{r}
data<-algas
print(data$a3[1])
```

```{r}
imputar_por_similitud <- function(data, num_vecinos)
{
 for(variable in colnames(data) )
 {
     if( is.numeric( data[[variable]][1] ) )
     {
         print(paste('Variable Numerica: ' , variable))

         var_media<-mean( unlist( data[variable] ), na.rm=TRUE )
         var_sd<- sd( unlist( data[variable] ), na.rm=TRUE)
         
         cat(paste(paste(paste('\nVariable Media:', var_media ), '    Variable sd: '), var_sd))
         
         variable_std<-(data[[variable]]-var_media ) /var_sd
         vecinos<- data[colnames(data)[!colnames(data) == variable]]
         lista_distancias<-list()
         #print(paste('COL NAMES:' , colnames(vecinos)))
         for(vecino in colnames(vecinos) )
         {
             
            if( is.numeric(data[[vecino]][1]))
             {
                 cat(paste('\n\nVecino Numerica: ' , vecino))
                 vecino_media<-mean( unlist( data[vecino] ), na.rm=TRUE)
                 vecino_sd<- sd( unlist( data[vecino] ), na.rm=TRUE)
                 #Calculamos la distancia
                 cat(paste(paste(paste('\n\t    Vecino Media:', vecino_media ), '    Vecino sd: '), vecino_sd))
                 vecino_std<-(data[[vecino]]-vecino_media) /vecino_sd 
                 
                 suma<-0
                 cont<-0
                 #Calculamos la distancia
                 #print( paste('len: ', length(vecino_std)))
                 for(i in 1:length(vecino_std))
                 { 
                     cont<-cont+1
                     if(!is.na(vecino_std[[i]]) &&  !is.na(variable_std[[i]]))
                     {
                     suma<- suma + ( (vecino_std[[i]]-variable_std[[i]]  )**2 )
                     #print(paste('SUMA:', suma))
                     }
                 }
                 distancia<-sqrt(suma/cont)
                 #print(paste('DISTANCIA:  ', distancia))
                 lista_distancias<-c(lista_distancias, distancia)
                 
                
                 
            }
            else
             {
                print(paste('Vecino No numerico: ', vecino))
                lista_distancias<-c(lista_distancias, 0)
                     
             }
         }
         
         if(length(lista_distancias)>0)
         {
         
         #cat(paste('\n\n\t\tlista de distancias: ', unlist(lista_distancias)))
             
         #print(unlist( sort( unlist(lista_distancias), index.return=TRUE )[2]))
         vecinos_cercanos_orderNum<-unlist( sort( -unlist(lista_distancias), index.return=TRUE )[2])[1:num_vecinos]
         #print(paste('ORDERNUM: ', vecinos_cercanos_orderNum))
         vecinos_cercanos<-vecinos[vecinos_cercanos_orderNum]

         total<-sum(unlist(lista_distancias)[1:num_vecinos])
         ponderaciones<-list()
         suma<-0
         for(x in 1:num_vecinos)
        {
            ponderaciones<-c(ponderaciones, unlist(lista_distancias)[x]/total)
         }
         
         print(paste('PONDERACIONES: ', unlist(ponderaciones)))
         for(x in 1:length( vecinos_cercanos) )
         {
             #print(paste('VECINOS_CERCANOS: ',vecinos_cercanos[[x]] ))
             suma<-suma+ vecinos_cercanos[[x]][is.na(data[variable])] *  unlist(ponderaciones)[x]
         }
         
         
         print(suma)
         data[[variable]][ is.na(data[variable]) ]<-suma
        
         }
         else
         {
             print('No hay lista de distancias')
         }
     }
     else
     {
         print(paste('Variable NO Numerica: ' , variable))
         #No hay variables que sean similares para variables categoricas por lo que la delta de Kronecker siempre sera          #0
     }
 } 
    
}
```

- Aplícalo a `algas` y `german`. 

```{r, include=FALSE, cache=FALSE, eval=FALSE}
algas_sim<-imputar_por_similitud(algas, 2)
german_sim<-imputar_por_similitud(german_data, 2)
#imputar_por_similitud(german,2)

```
- ¿Son muy diferentes las estadísticas ignorando los `NAs` comparadas con este método?
* Esto dependera de que tantos valores tengamos asigandos en NA para cada Variable y que tan parecidos(que tan pequeñas sean las distancias) entre los valores que estamos usando para predecir los datos faltantes. En el caso de German Data y Algas los summary no se ven muy afectados porque no habia muchos valores NA. Asimismo en German Data la mayoria de las variables son no numericas y no se relacionan con las otras variables categoricas(No miden el mismo valor) luego entonces estas variables permanecieron iguales. 

```{r}
algas
summary(algas)
summar(algas_sim)

summary(german_data)
summary(german_sim)
```



**Ejercicio 19**

En este momento es quizá una buena idea, dejar de duplicar código y concentrar 
  todas las funciones de `utils.R` que se puedan reutilizar en un archivo `toolset.R`.
  Ajusta tus demás archivos de acordemente.
Crea un `R notebook` para cada *dataset* utilizando los archivos reproducibles y el archivo
  `toolset.R`. Incluye en estos `notebook` la estructura de los datos, *GEDA* transformaciones de los datos 
  y observaciones pertinentes (como *outliers*, estructura de los datos faltantes). 
  Explica los métodos de imputación que usaste (si fué necesario) y porqué los usaste.

*Se presentan todos los ejercicios en este R notebook con las funciones incorporadas para que sea más facil evaluar los resultados y comparar entre ejercicios.


***

## Titanic Data Set

**Ejercicio 20**

- En algunos *data sets* se agregaron columnas de más, remuévelas
- Genera las siguientes variables: `survived`, `name`, `last_name`, `sex`
- Agrega una columna de `age` que sea categórica
- Arregla la columna de precio, edad
- Ajusta a precios del día de hoy (Por ejemplo usa esta [página](http://inflation.stephenmorley.org/))¿En que clase hubieras viajado? ¿Cuál era tu probilidad de supervivencia?

De acuerdo con la calculadora de inflación 1 libra en 1912 equivale a 106 libras en valor presente. 106 libras son  25.5* 106 pesos mexicanos. 
```{r}

titanic['pesos_per_ticket_vp']<-titanic$pounds_per_ticket#*106*25.5

titanic %>% filter(Class %in% c('1st Class', '2nd Class', '3rd Class')) %>%
ggplot(aes(pesos_per_ticket_vp/1000, fill=survived)) + xlim(1,50)+
    geom_histogram() + facet_grid(sex~Class  )


#Si tomaramos solo los boletos que son economicamente accesibles es probable que terminara en 3ra clase. Dado que estoy en tercera clase tengo una probabilidad de sobrevivir de 24.4%
titanic_3rd<-subset(titanic, titanic$Class=='3rd Class')
print(length(titanic_3rd$survived[ titanic_3rd$survived] )/length(titanic_3rd$survived))

#Si ademas contemplamos que soy hombre entonces tendre una probabilidad de sobrevivencia de 15.6%
titanic_3rd_M<-subset(subset(titanic, titanic$Class=='3rd Class'),  sex=='M')
print(length(titanic_3rd_M$survived[ titanic_3rd_M$survived] )/length(titanic_3rd_M$survived))


```

- Crea un método para unificar los *data sets* en uno solo.
- Observando la distribución de botes que se muestra en la figura ¿Qué puedes decir sobre como se utilizaron?
  ¿Coincide con la película de Titanic de James Cameron?
En la grafica vemos qu ela mayoria de los barcos se encontraban a la mitad de su capacidad. Si no mal recuerdo en la pelicula no habia suficiente espacio para todos y los barcos estaban todos llenos. Hace mucho que la vi pero la distribución indica que los barcos 
```{r}
readRDS('titanic.RDS')
barcos<-titanic %>%
    group_by(Boat) %>%
    dplyr::summarise(n=n()) %>%
    arrange(desc(n))
barcos<-barcos %>% mutate( capacidad_total=ifelse( as.integer(Boat) %in% c(1:16), 65,
                                   ifelse (Boat %in% c(LETTERS[1:4]), 45, NA)))

ggplot( barcos, aes( n, capacidad_total, color=Boat )) + geom_point() + facet_grid(~  capacidad_total )+ xlim(1,65)
```


***
##Berka Data Set

```{sql eval=FALSE}
.import account.asc accounts
.import client.asc clients
.import card.asc cards
.import disp.asc disps
.import district.asc districts
.import loan.asc loans
.import order.asc orders
.import trans.asc transactions

 .save berka.raw
 
```

**Ejercicio 21**

Cargaremos los datos en `berka.raw`
```{sql}

sqlite3 berka.raw

```


**Ejercicio 22**

- Verifica que cada `account` tenga un `owner`
* Como podemos ver en esta base de datos cada cuenta tiene asignado un único cliente. Notemos que estamos aplicando la funcion count sobre account_id y client_id. De esta forma si hubiera mas de dos cientes con la misma cuenta o dos cuentas con el mismo cliente, count marcaria 2. Asimismo el ordenamiento descendente nos permitiria encontrar estos valores >1 al  inicio de la tabla.

```{sql }

select a.account_id, c.client_id, type, count(*) 
from accounts a, disps d, clients c  
on 
a.account_id = d.account_id 
and 
d.client_id = c.client_id 
group by a.account_id, c.client_id, type 
order by count(*) desc 
limit 10;

```


- Los registros de  `orders` y `loans` están duplicados en `transactions`. Es decir
los regsitros de `order`y `loan` están dentro de  `transactions` (por ejemplo, 
Los registros de `loan` en `tran` están identificados por el `k_symbol` `LP`)

```{r}

```

- Traduce los campos del checo al inglés.
- En `transaction` traduce la columna `type`, `operation`, `k_symbol`
- Ajustamos los nombres de las tablas a plural, los `*_id` a singular.
```{sql}
select distinct frequency from accounts;

update accounts set frequency='Weekly Fee' WHERE frequency='POPLATEK MESICNE';
update accounts set frequency='Monthly Fee' WHERE frequency='POPLATEK TYDNE' ;
update accounts set frequency='Fee after Transaction' WHERE frequency='POPLATEK PO OBRATU';

select distinct K_symbol from orders;

update orders set K_symbol='Premium' WHERE K_symbol='POJISTNE';
update orders set K_symbol='Sipo'    WHERE K_symbol='SIPO' ;
update orders set K_symbol='Leasing' WHERE K_symbol='LEASING';
update orders set K_symbol='Uber'    WHERE K_symbol='UBER';

select distinct type from cards;
Los valores estan en ingles.


select distinct type from transactions;
update transactions set type='INCOME' WHERE type='PRIJEM';
update transactions set type='EXPENDITURE' WHERE type='VYDAJ';
update transactions set type='CHOICE' WHERE type='VYBER';


```

- En `client` cambia `BirthNumber` a `sex` y `age`
- Discretiza `age` en `Youth` (0-24), `Adult` (25-45), `Middle-age` (46-64) y `Senior` (> 65)
```{sql}

SELECT 
(
CASE 
    WHEN year <= 1952 THEN 'Senior' 
    WHEN 1952 < year and year <= 1971  THEN 'Middle Age' 
    WHEN 1971 < year and year <= 1992 THEN 'Adult' 
    WHEN 1992 < year                   THEN 'Youth' 
    END
) age,
(
CASE 
    WHEN month >=50 THEN 'M' 
    WHEN month < 50 THEN 'F' 
    END
) sex,
year,
(
CASE
    WHEN month>=50 THEN (month-50)
    WHEN month<50 THEN month
    END
) month,
day

FROM
    (
    SELECT (substr(birth_number,1,2) +1900) year , 
        substr(birth_number,1,2) month , 
        substr(birth_number,1,2) day   
        FROM CLIENTS )

```


- En `disposition` cambia de `Dispondent` -> `User` 
```{sql}

select distinct type from disps;
update disps set type='USER' WHERE type='DISPONENT';
```

- En `loan` discretiza usando alguna heuristica `amount`, `duration`  y `payments`
```{sql}

SELECT 
(
CASE
    WHEN amount >= avg(amount) and duration>= avg(duration) and payments >= avg(payments) THEN 'Criterio 1'
    WHEN amount >= avg(amount) and duration>= avg(duration) and payments <  avg(payments) THEN 'Criterio 2'
    WHEN amount >= avg(amount) and duration < avg(duration) and payments >= avg(payments) THEN 'Criterio 3'
    WHEN amount >= avg(amount) and duration < avg(duration) and payments <  avg(payments) THEN 'Criterio 4'
    WHEN amount < avg(amount) and duration>= avg(duration) and payments >=  avg(payments) THEN 'Criterio 5'
    WHEN amount < avg(amount) and duration>= avg(duration) and payments <   avg(payments) THEN 'Criterio 6'
    WHEN amount < avg(amount) and duration<  avg(duration) and payments >=  avg(payments) THEN 'Criterio 7'
    WHEN amount < avg(amount) and duration<  avg(duration) and payments <   avg(payments) THEN 'Criterio 8'
    END
) criterios

FROM
(
    SELECT amount,
           duration,
           payments  
    FROM loans
)


```


- Guardemos los datos en `berka.clean`

```{r}
 .save berka.clean 
```


**Ejercicio 23**

- Denormalicemos los datos
- ¿Cuál es el objeto que nos interesa? ¿Tarjeta? ¿Transacción? ¿Cliente?
- ¿Cómo quedarían las tablas?
* Dependiendo de cual sea el objeto que más nos interesa analizar haremos el left_join con el resto de los variables empezando con la de prioridad uno hasta llegar a la de ultima prioridad. Siguiendo este metodo evitaremos perder datos reelevnates o que sean parte de nuestro analisis.
*  En el siguiente caso elegimos como variable principal clientes y juntamos las tablas de ordenes, cuentas y disps. La tabla de disps funciona como un conector entre clientes y accounts brindadndonos el account_id y el client_id que que le corresponde a cada account. LA siguiente estructura serviria para identificar los clientes con mayor numero de ordens y las caracteristicas de la cuenta de cada cliente. 
```{sql}

SELECT * FROM orders
LEFT JOIN
(
    SELECT * FROM clients
    LEFT JOIN
    (
        SELECT * FROM 
        accounts a INNER JOIN  disps on a.account_id
    ) b
    on b.client_id
    
)c on c.account_id

ORDER BY client_id;


```


***
##Models 
**Ejercicio 24 **
Implementar el método low_variability() en utils.r
```{r}

low_variability<-function( data, criterio=.20 )
{
  #La funcion obtiene un Data Set y elimina las variables con poca variabilidad.
  #Para definir que variables tienen poca variabilidad se obtine el QRT de cada una y se eliminan las que sean menor a rango.
  #rango elimina el 20% si no se define de otra manera

  suma<-0
  cont<-0
  for( var in data)
  {
     if(is.numeric( var[1]) )
     {
     suma<-suma+ IQR(unlist( var ), na.rm = TRUE)
     }

  }
  rango<- (suma/length(data)) * criterio #Promedio de IQRs por la ponceracion

  cat(paste('\n\nRango: ', rango ))

  for( var in names(data) )
  {
    if(is.numeric( data[[var]][1]))
    {
        if( IQR(unlist( data[var] ), na.rm = TRUE) < rango )
        {
            cat(paste( paste( '\nVariable con baja Variabilidad: ', var),
                   paste('\n\t\tVariabilidad == ' , IQR(unlist( data[var] ), na.rm = TRUE)  )))
            data[var]<-NULL
        }

    }
      else
      {
          print(paste('Variable no numerica: ', var ))
      }
  }

 return( data )
}


```

Aplicando la funcion con BOSTON Data Set
```{r}
print('BOSTON DATA SET')
data<-MASS::Boston
hightVarData<- low_variability(data)
cat('\n\nALGAS DATA SET')
data<-readRDS('algas.rds')
hightVarData<- low_variability(algas, .5)

```

**Ejericicio 25**
Implementar el método correlation_filtering() en utils.r.
Implementar el método correlation_filtering() en utils.py.

R:
```{r}
correlation_filtering<- function( data, criterio=.9 )
{
  listaSave<-c('')
  listaEliminar<-list()
  for( var in names(data) )
  {
    #print(paste('NAME: ', var))
    if(is.numeric(data[[var]][1]))
    {
      
    for(varComp in names(data))
    {
      if(is.numeric(data[[varComp]][1]))
      {   
        if(var!=varComp && !varComp %in% listaSave ) #Queremos comparar entre variables
        {
            #cat(paste(paste('\nCOMPAREE, correlacion',cor(data[var], data[varComp],  use="complete.obs") ), criterio, sep=' : '))
            
            if(abs(cor(data[var], data[varComp],  use="complete") )>criterio)
            {
            #cat('\n\tCORRELATION')
            print(paste(paste(paste('Variable ', var ), ' altamente correlacionada con '), varComp))
            listaEliminar<-c(listaEliminar, varComp)
            listaSave<-c(listaSave, var)
            }
        }
      }
    }
    }
    else
     {
     cat(paste('\nVariable no numerica: ', var))     
     }
  }
  listaEliminar<-unique(listaEliminar)
  cat(paste('\n\tVariables a eliminar: ', listaEliminar))
  cat(paste('\n\tVariables Seleccionadas: ', listaSave))
  for(i in listaEliminar)
  {
    data[i]<-NULL
  }

 return( data )
}

```

PYTHON:
```{python}


```

**Anotaciones: En el caso del data set algas las correlaciones aparecen muy bajas porque se ignoran todos los va,ores que son NA. Lo curioso es que son NA es ambos casos por ejemplo la correlacion de PO4 y oPO4 es de .287 pero es probable que las variables tengan una correlación mayor si asignaramos un valor cada vez que ambas variables tienen asignado NA en la misma posición.
```{r}

cat('\nBOSTON DATA SET')
data<-MASS::Boston
hightVarData<- correlation_filtering(data)

cat('\n\nALGAS DATA SET')
algas<-readRDS('algas.rds')
hightVarData<- correlation_filtering(algas, .5)

```



**Ejericicio 26**
Implementar el método FCB_filtering() en utils.r.
Implementar el método FCB_filtering() en utils.py.

```{r}
FCB_filtering<- function( y, data, criterio=.9 )
{
    listaSave<-c('')
    corrY<-list()
    variables<-list()

    for(i in names(data))
    {
        if( is.numeric(data[[i]][1] ))
        {
            variables<-c(variables, i )
            corrY<-c(  corrY, abs(cor (y, data[i], use='complete')))
            cat(paste(paste(paste('\nCorrelación: ', abs(cor(y, data[i], use='complete')) ), '  Variable:'), i))
        }
    }

    variables<-unlist(variables)
    corrY<-unlist(corrY)
    variables<-variables[order(corrY, decreasing = TRUE)]
    cat('\n\n\tVariables Ordenadas')
    print(variables)
    listaEliminar<-list()
    for( var in variables )
    {
        listaSave<-c(listaSave, var)
        for(varComp in names(data))
        {
            
            if(var!=varComp  && is.numeric(data[[varComp]][1] ) && !varComp %in% unlist(listaSave) )
            {
                if(cor(data[var], data[varComp], use = 'complete') >criterio)
                { 
                    listaEliminar<-c(listaEliminar, varComp)
                    
                }
            }
        }

    }

    listaEliminar<-unique(listaEliminar)
    print('lista a Eliminar')
    print(unlist(listaEliminar))
    print('Lista Guardada')
    print(unlist(listaSave))
    for(i in listaEliminar)
    {
        data[i]<-NULL
    }

    return( data )
}
```


```{r}
cat('\nBOSTON DATA SET')
data<-MASS::Boston
hightVarData<- FCB_filtering(data$crim ,select( data, -crim), .9)

cat('\n\nALGAS DATA SET')
algas<-readRDS('algas.rds')
hightVarData<- FCB_filtering(algas$PO4, select(algas, -PO4), .4)

```


**Ejericicio 26**

Implementar el método forward_filtering() en utils.r.
Implementar el método forward_filtering() en utils.py.

* Para R se propone el modelo de regresión lineal
```{r}

forward_filtering<-function(y, data)
{
    smp_size <- floor(0.75 * nrow(data))

    set.seed(123)
    train_ind <- sample(seq_len(nrow(data)), size = smp_size)
    X_train <- data[train_ind, ]
    X_test <- data[-train_ind, ]
    Y_train<- y[train_ind]
    Y_test <- y[-train_ind]
    
    
    
    for(var in colnames(data))
    {
        
    }
    
}

```



**Ejercicio 27**
Implementar el método epsilon() en utils.r, para el caso de una variable, tanto numérico como categórico.
Haga lo mismo para utils.py

**Ejercicio 28**
Crea una función para crear la curva ROC, llámala plot_roc y guárdala en utils.py

**Ejercicio 29**
Crea una función para generar la gráfica de precision/recall, llámale plot_prec_rec

**Ejercicio 30**
Crea una función para generar esta gráfica, llámale plot_double_density

**Ejercicio 31**
Abre Rstudio y guarda el data.frame de Titanic y guárdalo como archivo feather y en una base de datos sqlite.

**Ejercicio 32** 
Lee desde python la base de datos de Titanic.

**Ejercicio 33**
Crea tu versión del magicloop, pero agrega un parámetro para usar Grid Seach o Random Grid Search, utiliza cross-validation, no hold-out.

**Ejercicio 34**
Utilízalo para el dataset de Titanic. Utiliza los siguientes clasificadores: Random Forest, Logistic Regression, Extra Trees, AdaBoost, Logistic Regression, SVC, Naïve Bayes, Decision Trees, DummyClassfier y KNN.

**Ejercicio 35**
Reporta la métrica que vas a usar y el porqué la escogiste. ¿Cuál es el mejor modelo en esa métrica? ¿Utilizaste algún threshold? Presenta las gráficas de prec/rec@k y ROC.

**Ejercicio 36**
Compara los tiempos de GridSearch y Random Grid Search (Usa %timeit)

**Ejercicio 37**
Crea un servicio web que despliegue el mejor modelo de Titanic. Este servicio debe de recibir un vector de datos que y regrese la probabilidad de supervivencia (si aplica).
¿Cómo modificarías el servicio web que creaste para implementar evaluación en línea y que además permita ejecutar un algoritmo multiarmed bandit?
